\documentclass{article}

% Language setting
% Replace `english' with e.g. `spanish' to change the document language
\usepackage[english]{babel}

% Set page size and margins
% Replace `letterpaper' with `a4paper' for UK/EU standard size
\usepackage[letterpaper,top=2cm,bottom=2cm,left=3cm,right=3cm,marginparwidth=1.75cm]{geometry}

% Useful packages
\usepackage{amsmath}
\usepackage{graphicx}
\usepackage{float}


\usepackage[colorlinks=true, allcolors=blue]{hyperref}
\usepackage{amssymb}


\usepackage{minted}


\title{Backpropagation}
\author{Nicola Santi}

\begin{document}
\maketitle

\begin{abstract}
We present the most well-known derivation method in deep learning in an accessible way, building intuition on a simplified model, with the help of a numerical example and even some graphs.
\end{abstract}

\section{Introduction}

Avremo molte semplificazioni, ad esempio non esiste attivazione sull'ultimo livello.

\section{Multilayer dense network}
A dense feedforward neural network can be formalized as a sequence of $\displaystyle L$ layers, each of which receives an input vector $\displaystyle \mathbf{z} \in \mathbb{R}^{M}$ to which apply first a linear transformation $\displaystyle W$ and, then, a nonlinear transformation $\displaystyle h()$. The result is used by the next layer as input (hence the feed-forward meaning), see Figure \ref{fig:multi-layer-network}.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\linewidth]{images/multi layers network.png}
    \caption{Simple feed-forward network}
    \label{fig:multi-layer-network}
\end{figure}

The linear transformation inside each layer multiplies the input vector $\displaystyle \mathbf{z}$ by the model parameters $\displaystyle K\times M$ matrix $\displaystyle W$ (each layer has its own matrix with different dimensions $M$ and $K$) so that each input neuron (each component of vector $\displaystyle \mathbf{z}$) is connected to each output neuron (component of vector $\displaystyle \mathbf{a}$), hence the dense attribute of this kind of layers. The result is the vector $\displaystyle \mathbf{a} \in \mathbb{R}^{K}$.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.6\linewidth]{images/activation functions.png}
    \caption{https://www.researchgate.net/figure/Common-activation-functions-in-artificial-neural-networks-NNs-that-introduce_fig7_341310767}
    \label{fig:activations}
\end{figure}

A potentially non-linear transformation called activation function (or inverse link) is applied to vector $\displaystyle \mathbf{a}$. Common examples are sofmax, ReLU or Sigmoid functions (see Figure \ref{fig:activations}). In turn, the result of this activation function becomes the input of the next layer $\displaystyle \mathbf{z}^{( l)} =h\left(\mathbf{a}^{( l)}\right)$. The superscripts in parentheses indicate the layer to which each variable refers.

Summarizing and formalizing, for each layer $\displaystyle l$ we have:

\begin{gather*}
\mathbf{a}^{( l)} =W^{( l)}\mathbf{z}^{( l-1)}\\
\mathbf{z}^{( l)} =h^{( l)}\left(\mathbf{a}^{( l)}\right)
M^{( l)}=K^{( l-1)} \
\end{gather*}

Please, note that:
\begin{itemize}
    \item $\displaystyle \mathbf{x} =\mathbf{z}^{( 0)}$   is the input of the whole neural network;
    \item $\displaystyle \mathbf{y} =\mathbf{z}^{( L)}$  is the output of the whole neural network;
    \item $\displaystyle L$ is the total number of layers;
    \item $\displaystyle M^{( l)}$ is the dimension of layer $l$ input $\displaystyle \mathbf{z}^{( l-1)} \in \mathbb{R}^{M^{( l)}}$;
    \item $\displaystyle K^{( l)}$ is the dimension of the layer $l$ output $\displaystyle \mathbf{z}^{(l)} \in \mathbb{R}^{K^{( l)}}$;
    \item $\displaystyle K^{( l-1)}=M^{(l)} \ $ obviously, the size of the output of each level must match the next layer input;
\end{itemize}
\ \\

The superscripts in parentheses, as said, indicate the layer to which each variable refers: they are somehow inconvenient, so, for simplicity, in case of omission it will be understood that we are referring to the current level $\displaystyle l$. So we can simplify our equations:

\begin{gather*}
\mathbf{a} =W\mathbf{z}^{( l-1)}\\
\mathbf{z} =h(\mathbf{a})\\
M=K^{( l-1)} \
\end{gather*}



\begin{minted}{python}
model = keras.Sequential([
    layers.Input(shape=(2,)),
    layers.Dense(3, activation='tanh',   use_bias=False),
    layers.Dense(2, activation='linear', use_bias=False)
])
\end{minted}



\section{Three preliminary observations}

The backpropagation method aims to determine the derivative of the error function $\displaystyle E_{n}(\mathbf{y}) =\frac{1}{2}\sum ( y_{in} -t_{in})^{2}$ with respect to each model parameter $\displaystyle w_{k,m}^{( l)}$, for each $\displaystyle k$, $\displaystyle m$ e $\displaystyle l$.

In this section, we will try to understand how $\textbf{a}$ varies with respect to the variables of the same layer ($W$ and $\textbf{z}$) and, subsequently, with respect to the tensor $\textbf{a}^{l-1}$ of the previous layer.

We will discover that we are dealing with very simple derivatives, given the extreme linearity of the model. I recommend following the calculations in the figure below.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\linewidth]{images/last-layers details.png}
    \caption{Derivatives of $\textbf{a}$ with respect to the current layer $L$ and the previous one}
    \label{fig:last-details}
\end{figure}

\subsection{Derive from same layer}

In each layer $\displaystyle l$ we have two simple derivatives to deal with:

\begin{gather}
\frac{\partial a_{k}}{\partial z_{m}^{( l-1)}} \ =\ w_{k,m} \label{eq:from-z} \ \ \\
\notag\\
\frac{\partial a_{k}}{\partial w_{k,m}} \ =\ z_{m}^{( l-1)} \label{eq:from-w}\ \
\end{gather}

Taking as a reference the circled elements in Figure \ref{fig:last-details} : we know that $\displaystyle a_{2} =w_{2,1} z_{1} +\ w_{2,2} z_{2} +\ w_{2,3} z_{3}$ so derivative of $\displaystyle a_{2}$ from $\displaystyle w_{2,3}$ is simply $\displaystyle z_{2}$. Or, conversely, the derivative of $\displaystyle a_{2}$ from $\displaystyle z_{2}$ is  $\displaystyle w_{2,3}$.

\textbf{Spoiler alert}: the \eqref{eq:from-z}, on a closer inspection, already connects to the previous layer being the input of layer $\displaystyle l$ the output of the previous one but it is not yet the derivative with respect to $\textbf{a}^{l-1}$ that we will calculate in the next paragraph.\\

\begin{figure}[H]
    \centering
    \includegraphics[width=1\linewidth]{images/example.png}
    \caption{A numerical example}
    \label{fig:example}
\end{figure}

\textbf{Our example:} to refine the understanding, let us use the small two-layer neural network drawn above. For simplicity, we omit the final activation function $h^{L}$ assuming it is simply the identity function: $\displaystyle h^{L}(\textbf{a}) =\textbf{a}$. The weights displayed are those at a certain epoch in training.


\begin{gather}
\frac{\partial a_{2}}{\partial w_{2,3}} \ =\ z_{3}^{( 1)} =0.93 \notag  \\
\notag\\
\frac{\partial a_{2}}{\partial z_{3}^{( 1)}} \ =\ w_{2,3} \ =\ 0.6 \notag
\end{gather}

\subsection{Derive from previous layer}
As noticed, the \eqref{eq:from-z} already links current layer to previous one, because $\displaystyle \mathbf{z}^{( l-1)}$ is both previous layer output and current layer input. But we need do be linked to previous $\textbf{a}^{l-1}$. Remember that$\displaystyle \ z_{m}^{( l-1)} =h^{( l-1)}\left( a_{m}^{( l-1)}\right)$, so:


\begin{gather}
\frac{\partial z_{m}^{( l-1)}}{\partial a_{m}^{( l-1)}} \ =\dot{h}^{( l-1)} \label{eq:prev}
\end{gather}

This, obviously, involves the derivative of activation function $h^{l-1}()$. \\

\textbf{Our example:} using again Figure \eqref{fig:example}, we calculate the derivative of the third component of the first-level output $z_{3}^{( 1)}$ (which is also the third component of the second-level input) based on the $a_{3}^{( 1)}$.

\begin{equation*}
\frac{\partial z_{3}^{( 1)}}{\partial a_{3}^{( 1)}} \ =\dot{tanh}( 1.7) =1-tanh^{2}( 1.7) =1-( 0.93)^{2}
\end{equation*}

\section{Backpropagation algotrithm}
The basic idea of backpropagation is to compute the derivative (more precisely the gradient) of each loss function $\displaystyle E_{n}(\mathbf{y}) =\frac{1}{2}\sum ( y_{in} -t_{in})^{2}$ with respect to all model parameters $w_{k,m}$, moving in reverse from the last layer to the first by reusing the calculations already made during the forward way.

The idea is to calculate the derivative of $E$ with respect to all $a$ starting from the last level  $L$ and proceeding backwards using \eqref{eq:from-z} and \eqref{eq:prev}. At that point, we have all the information we need to calculate, using \eqref{eq:from-w}, the derivatives with respect to each $W$ final objective of our algorithm.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\linewidth]{images/backprop.png}
    %\caption{Enter Caption}
    \label{fig:enter-label}
\end{figure}

Let's formalise our algorithm: the only two steps to be repeated for each layer:
\begin{enumerate}
    \item We derive $E$ with respect to last layer $\textbf{a}^{L}$;
    \item We calculate the derivative of $\textbf{a}^{L}$ with respect to $W^{L}$ using \eqref{eq:from-w};
    \item Using \eqref{eq:from-z} and \eqref{eq:prev} we move to previous layer and derive $\textbf{a}^{L}$ with respect to $\textbf{a}^{L-1}$
    \item Repeat steps 2 and 3 until we reach the first layer
\end{enumerate}


\subsection{Last layer}

We begin by noting how the derivative of $E(W)$ with respect to the last level has an understandable meaning (remembering that $\textbf{y}=\textbf{a}^{(L))}$):
\begin{equation*}
\frac{\partial E}{\partial a_{k}^{( L)}} =\frac{\partial \frac{1}{2}\sum ( a_{i} -t_{i})^{2}}{\partial a_{k}^{( L)}} =\ y_{k} -t_{k}
\end{equation*}

This is the error of the component $\displaystyle k$: for convenience of notation, we can call $\displaystyle \delta _{k}^{( l)}$ simply error.


\begin{equation}
\delta _{k}^{( l)} \equiv \frac{\partial E}{\partial a_{k}^{( l)}} \label{eq:error}
\end{equation}

\ \\

\textbf{Our example:} We apply step 1 to calculate the error $\delta^{L}$ of our example described in Figure \ref{fig:example}:

\begin{gather*}
\delta _{1}^{( 2)} =\frac{\partial E}{\partial a_{1}^{( 2)}} \ =\ ( 0.48-1) \ =-0.52\ \ \\
\\
\delta _{2}^{( 2)} =\frac{\partial E}{\partial a_{2}^{( 2)}} \ =\ ( 1.14-1.15) \ =-0.01
\end{gather*}

We complete the last layers $L$d by deriving with respect to each element of matrix $W$:


\begin{gather*}
\frac{\partial E}{\partial w_{1,1}^{( 2)}} \ =\ 0.46\ ( -0.52\ \ ) ;\ \ \ \frac{\partial E}{\partial w_{1,2}^{( 2)}} \ =\ 0.8\ ( -0.52\ \ ) \ ;\ \ \ \frac{\partial E}{\partial w_{1,3}^{( 2)}} \ =\ 0.93\ ( -0.52\ \ )\\
\\
\frac{\partial E}{\partial w_{2,1}^{( 2)}} \ =\ 0.46\ ( -0.01\ \ ) ;\ \ \ \frac{\partial E}{\partial w_{2,2}^{( 2)}} \ =\ 0.8\ ( -0.01\ \ ) \ ;\ \ \ \frac{\partial E}{\partial w_{2,3}^{( 2)}} \ =\ 0.93\ ( -0.01\ \ )
\end{gather*}

\subsection{Previous layer}

 Proceeding back to the level $L-1$, we should note how the modification of $a_{m}^{(L-1)}$ impacts on each $a_{k}^{(L)}$. With the help of Figure \ref{fig:example}, let us convince ourselves that each element $k$ of $\textbf{a}^{L}$ is in fact modified by $a_{m}^{(L-1)}$ because it equals $\ a_{k} =... +\ w_{k,m} z_{m} +...$. In other words, each row of the matrix $W$ is multiplied by $\textbf{z}^{L-1}$. Please note how the variation is weighted by the coefficient $w_{k,m}$.

Thus, using the usual rules of derivation, we obtain that:

\begin{gather*}
\delta _{m}^{( L-1)} =\frac{\partial E}{\partial a_{m}^{( L-1)}} \ =\sum _{k}^{K^{( L)}}
\underbrace{\frac{\partial z_{m}^{( L-1)}}{\partial a_{m}^{( L-1)}}}_{\eqref{eq:prev}} \
\underbrace{\frac{\partial a_{k}^{( L)}}{\partial z_{m}^{( L-1)}}}_{\eqref{eq:from-z}} \
\underbrace{\frac{\partial E}{\partial a_{k}^{( L)}}}_{\eqref{eq:error}}\\
\\
=\dot{h}^{( L-1)}\left( a_{m}^{( L-1)}\right)\sum _{k}^{K^{( L)}} \ w_{k,m}^{( L)} \ \delta _{k}^{( L)} \ \ \\
\end{gather*}

We have used the \eqref{eq:prev}, \eqref{eq:from-z} and \eqref{eq:error} where indicated.

\textbf{Our example:} I think it may be useful to train intuition with the simple calculations for the penultimate $L-1$ layer (which, in our example, is also the first). Starting with $\boldsymbol{\delta^{L-1}}$


\begin{gather*}
\delta _{1}^{( 1)} =tan\dot{h}( 0.5)[ \ 0.1\ ( -0.52) +\ 0.4( -0.01)] =-0.044\\
\delta _{2}^{( 1)} =tan\dot{h}( 1.1)[ \ 0.2\ ( -0.52) +\ 0.5( -0.01)] =-0.039\\
\delta _{3}^{( 1)} =tan\dot{h}( 1.7)[ \ 0.3\ ( -0.52) +\ 0.6( -0.01)] =-0.020
\end{gather*}

Now compute derivation respect to $W^{L-1}$

\begin{gather*}
\frac{\partial E}{\partial w_{1,1}^{( 1)}} \ =\ 1\ ( -0.044) ;\ \ \frac{\partial E}{\partial w_{1,2}^{( 1)}} \ =\ 2\ ( -0.044) ;\ \\
\\
\frac{\partial E}{\partial w_{2,1}^{( 1)}} \ =\ 1\ ( -0.039) ;\ \ \frac{\partial E}{\partial w_{2,2}^{( 1)}} \ =\ 2\ ( -0.039) ;\ \\
\\
\frac{\partial E}{\partial w_{3,1}^{( 1)}} \ =\ 1\ ( -0.020) ;\ \ \frac{\partial E}{\partial w_{3,2}^{( 1)}} \ =\ 2\ ( -0.020) ;\
\end{gather*}


We thus calculated the derivatives with respect to all weights using only values already calculated during normal operation of the network, from the first layer to the last.

\subsection{Formula}
Quindi, gli errori $\displaystyle \delta _{k}^{( L)}$ e i pesi $\displaystyle w_{k,m}^{( L)}$ del livello successivo sono usati per calcolare l'errore del livello precedente, quindi sono propagati a ritroso rispetto al normale funzionamento di una rete neurale ff. Conviene notate come tutti i valori necessari sono gi√† stati calcolati durante il feed forward.

Per completezza riportiamo la formula generale, senza sorprese:


\begin{equation}
\delta _{m}^{( l-1)} \equiv \frac{\partial E}{\partial a_{m}^{( l-1)}} \ =\begin{cases}
( y_{m} -t_{m}) & l=L\\
\dot{h}^{( l-1)}( a_{m})\sum _{k}^{K^{( l)}} \ w_{k,m}^{( l)} \ \delta _{k}^{( l)} \ \  & l< L
\end{cases} \ \ \ \ \ \ \ \ \ \ m\in \left[ 1,K^{( l-1)}\right]
\end{equation}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.5\linewidth]{images/forward.png}
    \caption{Enter Caption}
    \label{fig:enter-label}
\end{figure}

\nocite{*}
\bibliographystyle{alpha}
\bibliography{sample}

\end{document}