\documentclass{article}

% Language setting
% Replace `english' with e.g. `spanish' to change the document language
\usepackage[english]{babel}

% Set page size and margins
% Replace `letterpaper' with `a4paper' for UK/EU standard size
\usepackage[letterpaper,top=2cm,bottom=2cm,left=3cm,right=3cm,marginparwidth=1.75cm]{geometry}

% Useful packages
\usepackage{amsmath}
\usepackage{graphicx}
\usepackage{float}


\usepackage[most]{tcolorbox}

\usepackage[backend=biber, style=authoryear, sorting=ydnt]{biblatex}
\addbibresource{sample.bib}  


\usepackage[colorlinks=true, allcolors=blue]{hyperref}
\usepackage{amssymb}



\usepackage{minted}

\newcommand{\authornote}{
Mathematical principles behind it date back to \textcite{kelley1960} and one of the earliest known implementations of backpropagation can be traced back to \textcite{werbos1974}, an aerospace engineer who introduced the concept in his 1974 Ph.D. thesis at Harvard University titled "Beyond Regression: New Tools for Prediction and Analysis in the Behavioral Sciences." Werbos recognized that this algorithm could be applied not only to neural networks but to a broader range of predictive models. Despite this early insight, his work remained largely unnoticed by the mainstream AI community for nearly a decade.}


\title{Backpropagation primer with numerical example and Python script}
\author{Nicola Santi}

\usepackage{hyperref}
\usepackage[capitalize]{cleveref}  % subito dopo


\begin{document}
\maketitle

\begin{abstract}
A self-contained introduction to the well-known backpropagation algorithm illustrated step by step, providing the mathematical elements necessary for understanding and a numerical example with which to verify what has been learnt. A Python script with keras and tensorflow to verify the calculations performed completes the exposition.

\end{abstract}

\section{Introduction}
Backpropagation, short for 'backward propagation of errors,' is perhaps the most iconic algorithm of modern machine learning. Suffice it to say that until the mid-1980s, nobody believed in the possibility of training a multilayer network anymore! The story is very interesting and reveals how a mistaken belief can delay scientific development even by decades. \\

While the first simple artificial neural network dates back to \textcite{mcculloch43a}, it was in  \textcite{rosenblatt1962principles}  that the theory and hardware needed to implement the first neural network called perceptron was developed. The hardware needed to implement it is depicted in the photos below:

\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\linewidth]{images/perceptron.png}
\end{figure}


 Perceptron has several layers but only one trainable so, in modern terms, it is a one-layer neural network, obviously subject to major limitations formally demonstrated in \textcite{minsky69perceptrons}. And here we come to the exact moment when machine learning based on neural networks was in danger of being completely abandoned because the two authors firmly believed, although without providing proof, that the same limitations would also apply to neural networks with more than one layer (deep learning).

 The widespread belief led to a suspension of interest, funding and thus research for the period from 1970 to the first half of the 1980s. Among other things, researchers could not pursue the study of multilayer models because there was no way to train them: the techniques used by perceptron were not applicable except for networks with only one layer.

 Then, as sometimes happens, a series of innovations contributed to the revival of neural networks and, among these, the most relevant was the invention \footnote{\authornote} of the backpropagation algorithm by \textcite{rumelhart1986}. The activation and loss functions had already been made differential, and backpropagation allows the derivative (more precisely the gradient) of the loss function to be calculated by moving backwards from the errors (difference between the value predicted by the model and the actual value) and going up from the last layer to the first. The most relevant aspect, as we shall see in this article, is the possibility of reusing the calculations already made by the neural network in its way forward (from the first layer to the last) to calculate the gradient, which saves resources and makes it possible to handle also even very deep neural networks.\\

 We will begin our exposition by recalling the few elements of mathematics necessary for understanding, which are limited to the multiplication of matrices and vectors and the calculation of derivatives of linear functions. We will then formally define a multilayer neural network and focus on a numerical example with two layers. We will use what we have learned to obtain two simple and important derivatives to be used in the last paragraph to obtain the complete backpropagation algorithm. We will test our understanding by carrying out elementary calculations on our example model, and we will also write a small Python script to verify the correctness of our accounts.

Let us proceed, then \verb|:-)|



\section{Matrix multiplication (and little else)}\label{sec:matrix}
In this article, we will use the original representation of matrix multiplication $WZ=A$ you can see below:

\begin{figure}[H]
    \centering
    \includegraphics[width=0.3\linewidth]{images/matrix.png}
    \caption{Two matrix multiplication}
\end{figure}

The advantage is the immediate visualisation of the dimensions of the matrix obtained, which will have as many rows as $W$ and as many columns as $Z$. Remember also that the number of columns of $W$ must coincide with the number of rows of $Z$ for multiplication to occur.

Let us now look at the special case of multiplication between matrix and vector $W\textbf{z}=\textbf{a}$:

\begin{figure}[H]
    \centering
    \includegraphics[width=0.5\linewidth]{images/matrix-vector.png}
    \caption{Matrix and vector multiplication}
    \label{fig:enter-label}
\end{figure}

It is important to note how each element of $\textbf{z}$ contributes to determining each element of $\textbf{a}$: this awareness will come in handy in one of the last paragraphs of this paper (\ref{sec:prev-layer}). Moreover, this is precisely why layers that contain a matrix multiplication are called dense, since each input element influences each output element.

Finally, each $a_k$ being the sum of products between elements of $\textbf{z}$ and $W$, the derivative of $a_k$ with respect to $z_m$ is simply $w_{k,m}$ while, symmetrically, the derivative with respect to $w_{k,m}$ is $z_m$ (see \ref{sec:same-layer}).

\section{Multilayer dense network: formalization}
A dense feedforward neural network can be formalized as a sequence of $\displaystyle L$ layers, each of which receives an input vector $\displaystyle \mathbf{z} \in \mathbb{R}^{M}$ to which apply first a linear transformation $\displaystyle W$ and, then, a nonlinear transformation $\displaystyle h()$. The result is used by the next layer as input (hence the feed-forward meaning), see Figure below.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\linewidth]{images/multi layers network.png}
    \caption{Simple feed-forward network}
    \label{fig:multi-layer-network}
\end{figure}

The linear transformation inside each layer multiplies the input vector $\displaystyle \mathbf{z}$ by the model parameter $\displaystyle K\times M$ matrix $\displaystyle W$ (each layer has its own matrix with different dimensions $M$ and $K$) so that each input neuron (each component of vector $\displaystyle \mathbf{z}$) is connected to each output neuron (component of vector $\displaystyle \mathbf{a}$). The result is the vector $\displaystyle \mathbf{a} \in \mathbb{R}^{K}$.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.6\linewidth]{images/activation functions.png}
    \caption{Some common activation functions}    
    
\end{figure}

A potentially non-linear transformation called activation function then is applied to vector $\displaystyle \mathbf{a}$. Common examples of this kind of functions are sofmax, ReLU or Sigmoid  (see Figure above from  \url{www.researchgate.net}). In turn, the result of this activation function becomes the input of the next layer $\displaystyle \mathbf{z}^{( l)} =h(\mathbf{a}^{( l)})$. The superscripts in parentheses indicate the layer to which each variable refers. 

Summarizing and formalizing, for each layer $\displaystyle l$ we have:

\begin{gather*}
\mathbf{a}^{( l)} =W^{( l)}\mathbf{z}^{( l-1)}\\
\mathbf{z}^{( l)} =h^{( l)}\left(\mathbf{a}^{( l)}\right)\\
M^{( l)}=K^{( l-1)} \ 
\end{gather*}

The loss function is simply the mean square of the errors $E_{n}(\mathbf{y}) =\frac{1}{2}\sum ( y_{mn} -t_{mn})^{2}$, where $n$ denotes a single data point.   \\

A couple of definitions and a few remarks conclude the presentation of our network: 
\begin{itemize}
    \item $\displaystyle \mathbf{x} =\mathbf{z}^{( 0)}$   is the input of the whole neural network;
    \item $\displaystyle \mathbf{y} =\mathbf{z}^{( L)}$  is the output of the whole neural network;
    \item $\displaystyle L$ is the total number of layers;
    \item $\displaystyle M^{( l)}$ is the dimension of layer $l$ input $\displaystyle \mathbf{z}^{( l-1)} \in \mathbb{R}^{M^{( l)}}$;
    \item $\displaystyle K^{( l)}$ is the dimension of the layer $l$ output $\displaystyle \mathbf{z}^{(l)} \in \mathbb{R}^{K^{( l)}}$;
    \item $\displaystyle K^{( l-1)}=M^{(l)} \ $ obviously, the size of the output of each level must match the next layer input;
\end{itemize}
\ 
\begin{tcolorbox}[colback=gray!10, colframe=gray!50, title=Note]
For didactic purposes, the last layer $L$ has no activation function or, and the same thing, uses the identity function $h(\textbf{a})=\textbf{a}$. We will return to this point in the section \ref{sec:last-layer}.
For similar reasons, each level has no bias.
\end{tcolorbox}

The superscripts in parentheses, as said, indicate the layer to which each variable refers: they are somehow inconvenient, so, for simplicity, in case of omission it will be understood that we are referring to the current level $\displaystyle l$. So we can simplify our equations:

\begin{gather*}
\mathbf{a} =W\mathbf{z}^{( l-1)}\\
\mathbf{z} =h(\mathbf{a})\\
M=K^{( l-1)} \ 
\end{gather*}

\textbf{Python code:} we can declare the model just described with a few lines of Python using the well-known keras library.

\begin{minted}{python}
model = keras.Sequential([
    layers.Input(shape=(2,)),
    layers.Dense(3, activation='tanh',   use_bias=False),
    layers.Dense(2, activation='linear', use_bias=False)
])
\end{minted}

The source code is available at \url{github.com/nicolinux72/backpropagation.git}.

\section{Two simple, useful derivatives}

In this section, we will try to understand how $\textbf{a}$ varies with respect to the variables of the same layer ($W$ and $\textbf{z}$) and, subsequently, with respect to the tensor $\textbf{a}^{l-1}$ of the previous layer.

We will discover that we are dealing with very simple derivatives, given the extreme linearity of the model. I recommend following the calculations in the figure below.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\linewidth]{images/last-layers details.png}
    \caption{Derivatives of $\textbf{a}$ with respect to the current layer $L$ and the previous one}
    \label{fig:last-details}
\end{figure}


\subsection{From same layer}\label{sec:same-layer}

In each layer $\displaystyle l$ we have two simple derivatives to deal with:

\begin{gather}
\frac{\partial a_{k}}{\partial w_{k,m}} \ =\ z_{m}^{( l-1)} \label{eq:from-w}\ \ \\
\notag\\
\frac{\partial a_{k}}{\partial z_{m}^{( l-1)}} \ =\ w_{k,m} \label{eq:from-z} \ \
\end{gather}

Taking as a reference the circled elements in Figure \ref{fig:last-details} : we know from \ref{sec:matrix} that $\displaystyle a_{2} =w_{2,1} z_{1} +\ w_{2,2} z_{2} +\ w_{2,3} z_{3}$ so derivative of $\displaystyle a_{2}$ from $\displaystyle w_{2,3}$ is simply $\displaystyle z_{2}$. Or, conversely, the derivative of $\displaystyle a_{2}$ from $\displaystyle z_{2}$ is  $\displaystyle w_{2,3}$.\\

\textbf{Spoiler alert}: the \eqref{eq:from-z}, on a closer inspection, already connects to the previous layer being the input of layer $\displaystyle l$ the output of the previous one but it is not yet the derivative with respect to $\textbf{a}^{l-1}$ that we will calculate in the next paragraph.\\

\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\linewidth]{images/example.png}
    \caption{A numerical example}
    \label{fig:example}
\end{figure}

\textbf{Our example:} to refine the understanding, let us use the small two-layer neural network drawn above.  The weights displayed are those at a certain epoch in training, the label $\textbf{t}$ is the truth for a certain data point. All values are approximated to two decimal places:


\begin{gather}
\frac{\partial a_{2}}{\partial w_{2,3}} \ =\ z_{3}^{( 1)} =0.93 \notag  \\
\notag\\
\frac{\partial a_{2}}{\partial z_{3}^{( 1)}} \ =\ w_{2,3} \ =\ 0.6 \notag 
\end{gather}

\begin{center}\rule{0.9\linewidth}{0.3pt}\end{center}

\textbf{Python code:} possiamo ricreare il nostro esempio inizializzando i pesi di ogni livello, istanziando il tensore di $x$ di input e l'etichetta $t$.

\begin{minted}{python}
# set weights as in the article
w1 = np.array([[0.1, 0.2],
               [0.3, 0.4],
               [0.5, 0.6]])

w2 = np.array([[0.1, 0.2, 0.3],
               [0.4, 0.5, 0.6]])

model.layers[0].set_weights([w1.T])
model.layers[1].set_weights([w2.T])

# set also a fake data point and label
x = tf.constant([[1.0, 2.0]])
y_true = tf.constant([[1.0, 1.15]])
\end{minted}


\subsection{From previous layer}
As noticed, the derivative  \eqref{eq:from-z} already links current layer to previous one, because $\displaystyle \mathbf{z}^{( l-1)}$ is both previous layer output and current layer input. But we need do be linked to previous $\textbf{a}^{l-1}$. It's an easy task remembering that$\displaystyle \ z_{m}^{( l-1)} =h^{( l-1)}( a_{m}^{( l-1)})$, so:


\begin{gather}
\frac{\partial z_{m}^{( l-1)}}{\partial a_{m}^{( l-1)}} \ =\dot{h}^{( l-1)} (a_{m}^{(l-1)})\label{eq:prev} 
\end{gather}

This, obviously, involves the derivative of activation function $h^{l-1}()$. \\   

\textbf{Our example:} Again, using  Figure \eqref{fig:example}, we calculate the derivative of the third component of the first-level output $z_{3}^{( 1)}$ (which is also the third component of the second-level input) based on the $a_{3}^{( 1)}$. Remember that, in our example, the activation of layer $L-1$ is $tanh()$ :

\begin{equation*}
\frac{\partial z_{3}^{( 1)}}{\partial a_{3}^{( 1)}} \ =\dot{tanh}( 1.7) =1-tanh^{2}( 1.7) =1-( 0.93)^{2}
\end{equation*}

\begin{center}\rule{0.9\linewidth}{0.3pt}\end{center}

Then, using the \eqref{eq:from-z} and the \ref{eq:prev}, it is possible to obtain the derivative of $\textbf{a}$ with respect to the previous $\textbf{a}^{(l-1)}$:

\begin{gather}
\frac{\partial a_{k}^{(l)} }{ \partial a_{m}^{( l-1)}}  
= \underbrace{\frac{\partial z_{m}^{( l-1)}}{\partial a_{m}^{( l-1)}}}_{\eqref{eq:prev}} \ 
\underbrace{\frac{\partial a_{k}^{( l)}}{\partial z_{m}^{( l-1)}}}_{\eqref{eq:from-z}} \
=\dot{h}^{( l-1)}( a_{m}^{( l-1)}) \ w_{k,m}^{( l)} \label{eq:a-prev}
\end{gather}\\


\textbf{Our example:} we simply apply the \eqref{eq:a-prev}


\begin{equation*}
\frac{\partial a_{2}^{( 2)}}{\partial a_{3}^{( 1)}} \  =[(1-( 0.93)^{2}]0.6
\end{equation*}



\section{Backpropagation algotrithm}
To train our model we need to compute the derivative (more precisely the gradient) of each loss function $\displaystyle E_{n}(\mathbf{y}) =\frac{1}{2}\sum ( y_{in} -t_{in})^{2}$ with respect to all model parameters $w_{k,m}$. 

\begin{figure}[H]
    \centering
    \includegraphics[width=0.6\linewidth]{images/backprop.png}
    \caption{Backpropagation move from layer to previuos one using $\textbf{a}$} derivatives and then compute gradient respect with $W$.   
    \label{fig:enter-label}
\end{figure}


The basic idea of backpropagation is to calculate the derivative of $E$ with respect to all $\textbf{a}$ starting from the last layer $L$ and proceeding backward using \eqref{eq:a-prev} , moving in reverse we could reuse the calculations already made during the forward way.   

Having obtained the derivatives with respect to each $\textbf{a}$, we have all the information needed to calculate, using \eqref{eq:from-w}, the derivatives with respect to each $W$ which are the final objective of our algorithm.\\

Let us formalize backpropagation algorithm in the easy steps below:
\begin{enumerate}
    \item Derive $E$ with respect to last layer $\textbf{a}^{L}$;
    \item Calculate the derivative of $\textbf{a}^{L}$ with respect to $W^{L}$ using \eqref{eq:from-w};
    \item Using \eqref{eq:a-prev} we move to previous layer $L-1$ and derive $\textbf{a}^{L}$ with respect to $\textbf{a}^{L-1}$
    \item Repeat steps 2 and 3 until we reach the first layer
\end{enumerate}

Let's start!

\subsection{Last layer}\label{sec:last-layer}

We begin by noting how the derivative of $E(W)$ with respect to the last layer has an understandable meaning (remembering that $\textbf{y}=\textbf{a}^{(L))}$):

\begin{equation*}
\frac{\partial E}{\partial a_{k}^{( L)}} 
=\frac{\partial \frac{1}{2}\sum ( a_{i} -t_{i})^{2}}{\partial a_{k}^{( L)}} 
=\frac{\partial \frac{1}{2}\sum ( y_{i} -t_{i})^{2}}{\partial a_{k}^{( L)}} 
=\ y_{k} -t_{k}
\end{equation*}

It is exactly error of the component $\displaystyle k$ so, for convenience of notation, we define $\displaystyle \delta _{k}^{( l)}$ and call it \textit{error}:


\begin{equation}
\delta _{k}^{( l)} \equiv \frac{\partial E}{\partial a_{k}^{( l)}} \label{eq:error} 
\end{equation}

\ \\

\begin{tcolorbox}[colback=gray!10, colframe=gray!50, title=Please note:]
As said, in our simplified model the last layer has no activation function: if there were, it would have to be taken into account in exactly the same way as \eqref{eq:prev} but the interpretation of $\delta$ as closely related to the error measure will remains valid.
\end{tcolorbox}


\textbf{Our example:} We apply step 1 to calculate the error $\delta^{L}$ of our example described in Figure \ref{fig:example}:

\begin{gather*}
\delta _{1}^{( 2)} =\frac{\partial E}{\partial a_{1}^{( 2)}} \ =\ ( 0.48-1) \ =-0.52\ \ \\
\\
\delta _{2}^{( 2)} =\frac{\partial E}{\partial a_{2}^{( 2)}} \ =\ ( 1.14-1.15) \ =-0.01
\end{gather*}
\begin{center}\rule{0.9\linewidth}{0.3pt}\end{center}

We complete the last layer by deriving $\textbf{a}$ with respect to each element $W$. We are helped by \eqref{eq:from-w} and \eqref{eq:error}.

\begin{gather*}
\frac{\partial E}{\partial w_{k,m}^{L}} 
= \frac{\partial E}{\partial a_{k}^{L}} \frac{\partial a_{k}^{L}}{\partial w_{k,m}} \ =\ \delta_k^{L} \ z_{m}^{( l-1)} \label{eq:from-w} 
\end{gather*}\\

\textbf{Our example:}  compute derivative for each  $w_{k,m}$ of last layer $L$


{\small
\begin{gather*}
\frac{\partial E}{\partial w_{1,1}^{( 2)}} \ =\ -0.52\ ( 0.46 ) = -0.24 \ ;\ \ \ 
\frac{\partial E}{\partial w_{1,2}^{( 2)}} \ =\ -0.52\ ( 0.80 ) = -0.41 \ ;\ \ \ 
\frac{\partial E}{\partial w_{1,3}^{( 2)}} \ =\ -0.52\ ( 0.93 ) = -0.48 \\
\\
\frac{\partial E}{\partial w_{2,1}^{( 2)}} \ =\ -0.01\ ( 0.46 ) = -0.00 \ ;\ \ \ 
\frac{\partial E}{\partial w_{2,2}^{( 2)}} \ =\ -0.01\ ( 0.80 ) = -0.01 \ ;\ \ \ 
\frac{\partial E}{\partial w_{2,3}^{( 2)}} \ =\ -0.01\ ( 0.93 ) = -0.01
\end{gather*}
}


\subsection{Previous layer}\label{sec:prev-layer}

 Returning to the level $L-1$, we should note how the modification of each $a_{m}^{(L-1)}$ impacts on each $a_{k}^{(L)}$ as anticipated in Paragraph \ref{sec:matrix}. With the help of Figure \ref{fig:example}, let us convince ourselves that each element $k$ of $\textbf{a}^{L}$ is in fact modified by $a_{m}^{(L-1)}$ because it equals $\ a_{k} =... +\ w_{k,m} z_{m} +...$. In other words, each row (vector) of the matrix $W$ is multiplied by $\textbf{z}^{L-1}$. Please note how the variation is weighted by the coefficient $w_{k,m}$. 

Thus, using the usual rules of derivation, we obtain that:

\begin{gather*}
\delta _{m}^{( L-1)} =\frac{\partial E}{\partial a_{m}^{( L-1)}} \ =\sum _{k}^{K^{( L)}}
\underbrace{\frac{\partial z_{m}^{( L-1)}}{\partial a_{m}^{( L-1)}}\frac{\partial a_{k}^{( L)}}{\partial z_{m}^{( L-1)}}}_{\eqref{eq:a-prev}} \ 
\underbrace{\frac{\partial E}{\partial a_{k}^{( L)}}}_{\eqref{eq:error}}\\
\\
=\dot{h}^{( L-1)}\left( a_{m}^{( L-1)}\right)\sum _{k}^{K^{( L)}} \ w_{k,m}^{( L)} \ \delta _{k}^{( L)} \ \ \\
\end{gather*}

We have used the \eqref{eq:a-prev} and \eqref{eq:error} where indicated.

\textbf{Our example:} I think it may be useful to train intuition with the simple calculations for the penultimate $L-1$ layer (which, in our example, is also the first). Starting with $\boldsymbol{\delta^{L-1}}$


\begin{gather*}
\delta _{1}^{( 1)} =tan\dot{h}( 0.5)[ \ 0.1\ ( -0.52) +\ 0.4( -0.01)] =-0.044\\
\delta _{2}^{( 1)} =tan\dot{h}( 1.1)[ \ 0.2\ ( -0.52) +\ 0.5( -0.01)] =-0.039\\
\delta _{3}^{( 1)} =tan\dot{h}( 1.7)[ \ 0.3\ ( -0.52) +\ 0.6( -0.01)] =-0.020
\end{gather*}

Now compute the derivation with respect to $W^{L-1}$


{\small
\begin{gather*}
\frac{\partial E}{\partial w_{1,1}^{( 1)}} \ =\ -0.044\ ( 1 ) = -0.04 \ ;\quad 
\frac{\partial E}{\partial w_{1,2}^{( 1)}} \ =\ -0.044\ ( 2 ) = -0.09 \ ;\\
\\
\frac{\partial E}{\partial w_{2,1}^{( 1)}} \ =\ -0.039\ ( 1 ) = -0.04 \ ;\quad 
\frac{\partial E}{\partial w_{2,2}^{( 1)}} \ =\ -0.039\ ( 2 ) = -0.08 \ ;\\
\\
\frac{\partial E}{\partial w_{3,1}^{( 1)}} \ =\ -0.020\ ( 1 ) = -0.02 \ ;\quad 
\frac{\partial E}{\partial w_{3,2}^{( 1)}} \ =\ -0.020\ ( 2 ) = -0.04
\end{gather*}
}



We thus calculated the derivatives with respect to all weights using only values already calculated during forward navigation of the network, from the first layer to the last.



\subsection{Final recipe}
Thus, the errors $\displaystyle \delta _{k}^{( L)}$ and the weights $\displaystyle w_{k,m}^{( L)}$ of the next level are used to calculate the error of the previous level, so they are propagated backwards from the normal operation of an ff neural network. Notice how all the necessary values have already been calculated during the feed forward.

For the sake of completeness, we report the general formula which, after our journey, I hope will no longer be intimidating:


\begin{equation}
\delta _{m}^{( l-1)} \equiv \frac{\partial E}{\partial a_{m}^{( l-1)}} \ =\begin{cases}
( y_{m} -t_{m}) & l=L\\
\dot{h}^{( l-1)}( a_{m})\sum _{k}^{K^{( l)}} \ w_{k,m}^{( l)} \ \delta _{k}^{( l)} \ \  & l< L
\end{cases} \ \ \ \ \ \ \ \ \ \ m\in \left[ 1,K^{( l-1)}\right]
\end{equation}

\textbf{Python code:} We calculate the gradient using the self-differentiation of tensorflow.

\begin{minted}{python}
# setup auto-differentiation
with tf.GradientTape() as tape:
    y_pred = model(x)                                     # Forward pass
    loss = tf.reduce_mean(tf.square(y_true - y_pred))     # MSE (L2)

# Compute gradient
grads = tape.gradient(loss, model.trainable_variables)

# show gradients for our datapoint (direct)
print("--- Input:", x.numpy(), "\n")
print("--- Gradients")
print(f"Layer 1:\n{grads[0].numpy().T}")
print("Output:", model.layers[0](x).numpy(), "\n")
print(f"Layer 2:\n{grads[1].numpy().T}")
print("Output:", y_pred.numpy(), "\n")
print("--- Label:", y_true.numpy(), "\n")

\end{minted}

The output of the script confirms our calculations to an accuracy level of more than two decimal places.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.6\linewidth]{images/output.png}
    
\end{figure}

\section{Conclusions}

In this article we have tried to introduce the well-known backpropagation algorithm by providing the theoretical elements necessary for understanding and a numerical example with which to compare the results obtained. The presentation is completed by a simple Python script with keras and tensorflow available on github.


\nocite{*}
\printbibliography


\end{document}